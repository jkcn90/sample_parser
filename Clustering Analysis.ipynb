{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_file_path = \"data/sample_conversations.json\"\n",
    "\n",
    "with open(data_file_path) as data_file:\n",
    "    raw_data = json.load(data_file)\n",
    "    \n",
    "messages = [message['Text'] for datum in raw_data['Issues'] for message in datum['Messages']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 22264, n_features: 8480\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), stop_words='english',\n",
    "                             max_df=0.008, min_df=0.0001)\n",
    "X = vectorizer.fit_transform(messages)\n",
    "\n",
    "print(\"n_samples: %d, n_features: %d\" % X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#n_components = 5000\n",
    "\n",
    "#svd = TruncatedSVD(n_components)\n",
    "#lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "\n",
    "#X = lsa.fit_transform(X)\n",
    "#explained_variance = svd.explained_variance_ratio_.sum()\n",
    "#print(\"Explained variance of the SVD step: {}%\".format(int(explained_variance * 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0: welcome great day welcome great 6pm 9pm 9pm 6pm available flight available flights flights great let quickly great moment\n",
      "Cluster 1: 12a disappointed good time input valuable court 20th feb salazar hi salazar help\n",
      "Cluster 2: wonderful day wonderful zipcode great let insert great let service great let quickly great let pull great let know great let booked great ll\n",
      "Cluster 3: rest day rest good rest day good rest ok great enjoy great enjoy great enjoy rest enjoy enjoy rest day enjoy rest\n",
      "Cluster 4: perfect thank yiu yiu ok perfect ok perfect scheduled perfect scheduled scheduled sounds perfect emailing tickets emailing\n",
      "Cluster 5: afternoon earliest 6p return good afternoon schedule late busy good afternoon just afternoon just\n",
      "Cluster 6: welcome great day welcome great zipcode great moment great like great let service great let quickly great let pull great let know great let insert\n",
      "Cluster 7: thank help ok thank great let insert great ll great like great let service great let quickly great let pull great let know zipcode\n",
      "Cluster 8: nope zipcode great let booked great like great let service great let quickly great let pull great let know great let insert great let\n",
      "Cluster 9: running card running retry great scheduled appointment great scheduled scheduled appointment scheduled appointment going try try running card\n",
      "Cluster 10: ok thank zipcode great let booked great like great let service great let quickly great let pull great let know great let insert great let\n",
      "Cluster 11: ty great ty zipcode great let quickly great let pull great let know great let insert great let booked great let great just\n",
      "Cluster 12: looks looks like yup looks like working like working scheduled maintenance maintenance scheduled rebooted fixed\n",
      "Cluster 13: ok awesome awesome awesome scheduled flight cancelled cancelled ok awesome scheduled scheduled scheduled installation yes flight looking account\n",
      "Cluster 14: correct yes correct correct table open okay ahead just confirm did confirm did confirm did open did open account did open\n",
      "Cluster 15: help determine determine hello problem tv whats problem tv problem tv channels resetting hello problem tv channels working tv channels\n",
      "Cluster 16: today thanks 4202473659 acceptable ok sec 464 sec welcom ll thanks yes good thanks prompt\n",
      "Cluster 17: provide address phone provide address address phone address phone number great ll great let service great let quickly great let pull great let know great let insert\n",
      "Cluster 18: blah anybody ok work relieved visa ve great tanya kids 5185982457 8651234568\n",
      "Cluster 19: making appreciate help appreciate committed charges sure happy help thank thank appreciate easy great confirmed installation\n"
     ]
    }
   ],
   "source": [
    "n_clusters = 3000\n",
    "\n",
    "km = MiniBatchKMeans(n_clusters=n_clusters, n_init=1, init_size=10000, batch_size=5000)\n",
    "km.fit(X)\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(min(n_clusters, 20)):\n",
    "    print(\"Cluster %d:\" % i, end='')\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind], end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['If it is not working within 35-50min feel free to contact me again',\n",
       " 'Great, feel free to sumbit the HIt so long...',\n",
       " 'Seeing that you are a book lover, could we offer you a free book to help fill the bookcase?',\n",
       " 'Ok we have it. I will be sure to send that along with your free delivery. Is there anything else I can assist you with?',\n",
       " 'Bakr, this is werner. I apologize for this. Please feel free to submit the Hit, and just leave a note that we did not respond in sufficient time.',\n",
       " \"Feel free to contact us if the issue isn't resolved by then.\",\n",
       " \"Feel free to contact us again if you don't see it soon.\",\n",
       " 'I will send you a shelf for free',\n",
       " 'Ok great! I am going to send you a free shelf',\n",
       " 'If we cannot locate your modem or contact you within 24 hours  then please feel free to contact us back1',\n",
       " 'It retails for $675 but I can offer to you for free']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_predictions = km.predict(X)\n",
    "\n",
    "test_sentence = vectorizer.transform(['wifi help free'])\n",
    "#test_sentence = lsa.transform(test_sentence)\n",
    "\n",
    "test_sentence_prediction = km.predict(test_sentence)\n",
    "\n",
    "similar_messages = [message for i, message in enumerate(messages)\n",
    "                    if message_predictions[i] == test_sentence_prediction]\n",
    "similar_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#labels = km.labels_\n",
    "#metrics.silhouette_score(X, labels, metric='euclidean')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
